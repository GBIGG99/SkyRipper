{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Ug2C7GHUNO"
      },
      "outputs": [],
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ieA4poCNHdSY"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVCGxEikHgUc"
      },
      "source": [
        "# MatFormer Lab:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOG0dm7LHmBE"
      },
      "source": [
        " <table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/[Gemma_3n]MatFormer_Lab.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o6A5O7ofZTH"
      },
      "source": [
        "Gemma 3n is a multimodal, multilingual model from the Gemma family of models. You can read about Gemma 3n in [its docs](https://ai.google.dev/gemma/docs/gemma-3n) and [launch blog post](https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide). It is a unique model that is natively elastic, which means you have nested models! Gemma 3n was trained as a E4B model (effectively loads 4B parameters) with 35 layers and a 16,384 FFN hidden dimension. It has a E2B (30 layers and 8,192 FFN hidden dimension) nested within it and jointly trained as a MatFormer.\n",
        "\n",
        "[MatFormer](https://arxiv.org/abs/2310.07707) (ðŸª†Matryoshka Transformer) architecture is a novel nested transformer built for elastic inference. Think of it like Matryoshka dolls: a larger model contains smaller, fully functional versions of itself. This approach extends the concept of [Matryoshka Representation Learning](https://huggingface.co/papers/2205.13147) from just embeddings to all transformer components.\n",
        "\n",
        "Saving memory by having E2B nested within E4B is practically useful, but what makes MatFormer so powerful is its capability to smoothly span the entire Pareto-optimal accuracy-vs-model size cruve between E2B and E4B without any additional training. Using a simple technique called **Mix-n-Match** one can extract a model of any size between E2B and E4B from the main E4B model.\n",
        "\n",
        "Why would you slice a model? Given your specific deployment requirements, the E2B and E4B may not be the right fit. You may want to get an E3B, for example, which has quality higher than the E2B while requiring less compute than E4B.\n",
        "\n",
        "**In this notebook, you get to play with MatFormers and Mix-n-Match. You will specify which configuration you want for the submodel based on FFN dimension and skp layers, and then you will export the model to Hugging Face, enabling you to use with your favorite tools.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys5kfGnpHsDV"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YBXCVVERIGzM"
      },
      "outputs": [],
      "source": [
        "# @title Install dependencies\n",
        "# @markdown Run this cell to install all the required dependencies. In particular, you'll need Hugging Face `transformers` and `timm` versions that support Gemma 3n. Note that you may need to restart the notebook after executing the following cell.\n",
        "\n",
        "# Install a transformers version that supports Gemma 3n (>= 4.53)\n",
        "!pip install \"transformers>=4.53\" \"timm>=1.0.16\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GFh8AhruHeMl"
      },
      "outputs": [],
      "source": [
        "# @title Login to Hugging Face\n",
        "# @markdown This is required so you can push the model to Hugging Face. You also need to make sure you have access to the Gemma 3n model repositories.\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yKFl4n5cLF9J"
      },
      "outputs": [],
      "source": [
        "# @title Import and Export Options\n",
        "# @markdown The MatFormer Lab allows you to load a Gemma 3n 4B checkpoint (either pre-trained or instruct-tuned) and to slice it. Below, please specify:\n",
        "\n",
        "# @markdown * The original repository ID from the checkpoint in Hugging Face\n",
        "\n",
        "# @markdown * A local path where the model will be saved\n",
        "\n",
        "# @markdown * A name of a repository to push the new checkpoint to\n",
        "\n",
        "original_model_id = \"google/gemma-3n-E4B-it\" # @param [\"google/gemma-3n-E4B-it\", \"google/gemma-3n-E4B-pt\"]\n",
        "local_output_path = \"my_modified_gemma_3n_model\" # @param {type:\"string\"}\n",
        "push_hf_repo_id = \"username/test-submodel\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P16x_lxGCcgc"
      },
      "source": [
        "### Slicing configuration\n",
        "\n",
        "As part of the Gemma 3n release, we share optimal slicing configurations as a [Hugging Face dataset repository](https://huggingface.co/datasets/google/gemma3n-slicing-configs), although you can also explore with your own configurations below.\n",
        "\n",
        "Each configuration specifies:\n",
        "* The hidden dimensions of the FFN\n",
        "* Which layers, if any, to skip\n",
        "* A MMLU accuracy associated with the pre-trained checkpoints\n",
        "\n",
        "`layer-level` and `block-level` configurations are a result of varying the hidden dimensions of the FFN either at a `layer-level` (fine-grained) or at a `block-level` (4 local + 1 global layers).\n",
        "\n",
        "At a `layer-level`, we found that having the global layers (instead of local layers) have higher capacity helps with accuracy for the same model size.\n",
        "\n",
        "At a `block-level`, we found that the block skipped for E2B (ie., layers 20-24) would benfit from higher capacity when not skipped and earlier blocks can work well with lower-capacity compared to the later blocks.\n",
        "\n",
        "**We invite the community to find even better configurations that lie on the Pareto-optimal curve between E2B and E4B!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHdEebcGYats"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"hf://datasets/google/gemma3n-slicing-configs/configs.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldNQRKptGnbU"
      },
      "source": [
        "Based on your deployment scenarios, you may want to pick a different config. Select below your preferred one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BfNaTzifGa9x"
      },
      "outputs": [],
      "source": [
        "#@title Config details\n",
        "\n",
        "import ast\n",
        "\n",
        "config_name = \"Config for official E2B Model\"# @param ['Config for official E2B Model', 'Config for E1.96B (layer-level)', 'Config for E2.54B (layer-level)', 'Config for E2.69B (layer-level)', 'Config for E2.98B (layer-level)', 'Config for E3.18B (layer-level)', 'Config for E3.39B (layer-level)', 'Config for E3.59B (layer-level)', 'Config for E3.79B (layer-level)', 'Config for E2.49B (block-level)', 'Config for E2.73B (block-level)', 'Config for E2.98B (block-level)', 'Config for E3.24B (block-level)', 'Config for E3.49B (block-level)', 'Config for E3.79B (block-level)']\n",
        "\n",
        "def safe_string_to_list(value):\n",
        "    \"\"\"\n",
        "    Converts a string representation of a list into a Python list.\n",
        "    - Converts NaN/missing values to an empty list [].\n",
        "    - Uses eval() to handle expressions like '2_048 * 8'.\n",
        "    - Safely handles non-string values by returning them as is.\n",
        "    \"\"\"\n",
        "    # First, check if the value is missing (NaN, None, etc.)\n",
        "    if isinstance(value, list):\n",
        "        return value\n",
        "\n",
        "    # Priority 2: Now that we know it's not a list, check if it's a missing value.\n",
        "    if pd.isna(value):\n",
        "        return []\n",
        "\n",
        "    # Priority 3: If it's a string, try to evaluate it.\n",
        "    if isinstance(value, str):\n",
        "        try:\n",
        "            return eval(value)\n",
        "        except (SyntaxError, NameError):\n",
        "            return value  # Return invalid string as is\n",
        "\n",
        "    # Fallback for any other type (like an integer)\n",
        "    return value\n",
        "\n",
        "df['FFN Hidden Dims List'] = df['FFN Hidden Dims'].apply(safe_string_to_list)\n",
        "df['Layers Skipped'] = df['Layers Skipped'].apply(safe_string_to_list)\n",
        "\n",
        "df_indexed = df.set_index('name')\n",
        "model_row = df_indexed.loc[config_name]\n",
        "\n",
        "layers_to_skip = model_row['Layers Skipped']\n",
        "ffn_hidden_dims = model_row['FFN Hidden Dims List']\n",
        "ffn_hidden_dims_str = model_row['FFN Hidden Dims']\n",
        "\n",
        "print(config_name)\n",
        "print(\"\\nLayers Skipped:\")\n",
        "print(layers_to_skip)\n",
        "print(\"\\nFFN Hidden Dims:\")\n",
        "print(ffn_hidden_dims_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMY2cY6JY15G"
      },
      "source": [
        "If you want to set up your own configuration, please uncomment the following cell and assign values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Lip1avNY9xd"
      },
      "outputs": [],
      "source": [
        "# Custom config\n",
        "#\n",
        "# layers_to_skip = [] # e.g. [20, 21, 22, 23, 24]\n",
        "# ffn_hidden_dims = [] # e.g. [2048 * 4, ...]\n",
        "# ffn_hidden_dims_str = str(ffn_hidden_dims)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmZcNRV5IUmk"
      },
      "source": [
        "## Slicing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7r0gnOaIWO3"
      },
      "source": [
        "### Load the model config and verify slicing configuration\n",
        "\n",
        "Note: we do not load the model at this stage, just verify that the slicing configuration is possible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9qsekLDLPiv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "\n",
        "original_config = AutoConfig.from_pretrained(\"google/gemma-3n-E4B-it\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1ff248c"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pgVAGEHA3w4"
      },
      "outputs": [],
      "source": [
        "model_config = original_config.text_config\n",
        "\n",
        "num_layers = model_config.num_hidden_layers\n",
        "final_num_layers = num_layers - len(layers_to_skip)\n",
        "\n",
        "if len(ffn_hidden_dims) != final_num_layers:\n",
        "    raise ValueError(\n",
        "        f\"The length of ffn_hidden_dims ({len(ffn_hidden_dims)}) must be equal \"\n",
        "        f\"to the final number of layers ({final_num_layers}).\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pytVtFyqJPfp"
      },
      "source": [
        "### Update configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz8KQOEEBERT"
      },
      "outputs": [],
      "source": [
        "num_kv_comp_layers = model_config.num_hidden_layers - model_config.num_kv_shared_layers\n",
        "local_kv_sharing_layer_idx = num_kv_comp_layers - 2\n",
        "global_kv_sharing_layer_idx = num_kv_comp_layers - 1\n",
        "\n",
        "if (local_kv_sharing_layer_idx in layers_to_skip or global_kv_sharing_layer_idx in layers_to_skip):\n",
        "  raise ValueError(f'Layers {local_kv_sharing_layer_idx} and {global_kv_sharing_layer_idx} are reserved.')\n",
        "\n",
        "count_kv_sharing = sum(1 for layer in layers_to_skip if layer >= 20)\n",
        "model_config.num_kv_shared_layers -= count_kv_sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXEK9hdyBKA1"
      },
      "outputs": [],
      "source": [
        "count_activation_sparsity = sum(1 for layer in layers_to_skip if layer <= 9)\n",
        "activation_sparsity_list = [0.95] * (10 - count_activation_sparsity) + [0] * (\n",
        "    final_num_layers - 10 + count_activation_sparsity\n",
        ")\n",
        "model_config.activation_sparsity_pattern = activation_sparsity_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urMZZ2SLBOUr"
      },
      "outputs": [],
      "source": [
        "model_config.num_hidden_layers = final_num_layers\n",
        "model_config.intermediate_size = ffn_hidden_dims"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjRMQ97hJFtX"
      },
      "source": [
        "### Save the configuration and the unchanged tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGCI9MqoBP0w"
      },
      "outputs": [],
      "source": [
        "original_config.save_pretrained(local_output_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(original_model_id)\n",
        "tokenizer.save_pretrained(local_output_path)\n",
        "\n",
        "print(f\"New config saved to {local_output_path}\")\n",
        "print(f\"Final number of layers: {model_config.num_hidden_layers}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JlVceVlJVG9"
      },
      "source": [
        "### Load the model checkpoints\n",
        "\n",
        "Note: we are saving the model to disk, so there's no need to have a large CPU/GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qh_uISmBnVR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "model_path = snapshot_download(original_model_id, allow_patterns=[\"*.safetensors\"])\n",
        "safetensor_files = [os.path.join(model_path, f) for f in os.listdir(model_path) if f.endswith('.safetensors')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RHu0up8Jp6n"
      },
      "source": [
        "### Slice the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I2DvFSKB285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "a5c51782a30b4e92919d447e98bff14b",
            "a5f7f5c50ad74b2eaff0466862d27162",
            "7aa63c3408004f3e80b970afd57978b0",
            "b1f2419836ab47c6bea9292dc48dde2c",
            "7a0559c8842d4a3fbea4161adf86c918",
            "c9d0555e29df4926859fef8ac70e2e3a",
            "e333c06f3a2246859e0fd4ce66926201",
            "556d541d2cdf4b6a8a40f1268310cd97",
            "4d60e37b41cd43828c762d8746195c54",
            "0100bdec1fed489697e9f386c04600fc",
            "a837639fa7b944e992b9fee1033e0385"
          ]
        },
        "outputId": "350b54dd-412f-400d-a3ed-60935710b9f6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5c51782a30b4e92919d447e98bff14b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving shard model-00001-of-XXXXX.safetensors (size: 4.01 GB)\n",
            "Saving shard model-00002-of-XXXXX.safetensors (size: 5.48 GB)\n"
          ]
        }
      ],
      "source": [
        "from safetensors import safe_open\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "kept_layers_indices = [i for i in range(num_layers) if i not in layers_to_skip]\n",
        "layer_rename_map = {old_idx: new_idx for new_idx, old_idx in enumerate(kept_layers_indices)}\n",
        "\n",
        "# This will store the mapping of tensor names to the file they are saved in\n",
        "weight_map = {}\n",
        "\n",
        "# This will store tensors for the current shard we are building\n",
        "new_shard_state_dict = {}\n",
        "shard_counter = 1\n",
        "total_size = 0\n",
        "\n",
        "pbar = tqdm(total=len(safetensor_files), desc=\"Processing shards\")\n",
        "\n",
        "for shard_path in safetensor_files:\n",
        "    # Open a shard for streaming\n",
        "    with safe_open(shard_path, framework=\"pt\", device=\"cpu\") as f:\n",
        "        # Iterate over each tensor in the shard\n",
        "        for tensor_name in f.keys():\n",
        "            new_tensor_name = tensor_name\n",
        "            tensor = f.get_tensor(tensor_name)\n",
        "\n",
        "            # Case 1: Handle layer-specific parameters\n",
        "            match = re.search(r'\\.layers\\.(\\d+)\\.', tensor_name)\n",
        "            if match:\n",
        "                old_layer_idx = int(match.group(1))\n",
        "\n",
        "                # If this layer is meant to be skipped, we just continue to the next tensor\n",
        "                if old_layer_idx in layers_to_skip:\n",
        "                    continue\n",
        "\n",
        "                # Get the new sequential layer index\n",
        "                new_layer_idx = layer_rename_map[old_layer_idx]\n",
        "                new_tensor_name = tensor_name.replace(\n",
        "                    f'.layers.{old_layer_idx}.',\n",
        "                    f'.layers.{new_layer_idx}.'\n",
        "                )\n",
        "\n",
        "                # Get the target FFN dimension for this new layer\n",
        "                target_ffn_dim = ffn_hidden_dims[new_layer_idx]\n",
        "\n",
        "                # Check if this parameter is part of the FFN and needs slicing\n",
        "                if 'mlp.gate_proj.weight' in new_tensor_name or 'mlp.up_proj.weight' in new_tensor_name:\n",
        "                    # These layers project from model_dim -> ffn_hidden_dim.\n",
        "                    # We slice the output dimension (dim 0).\n",
        "                    tensor = tensor[:target_ffn_dim, :].contiguous()\n",
        "                elif 'mlp.down_proj.weight' in new_tensor_name:\n",
        "                    # This layer projects from ffn_hidden_dim -> model_dim.\n",
        "                    # We slice the input dimension (dim 1).\n",
        "                    tensor = tensor[:, :target_ffn_dim].contiguous()\n",
        "\n",
        "            # Case 2: Handle special non-layer parameters that need slicing\n",
        "            elif 'per_layer_model_projection' in tensor_name:\n",
        "                # Reshape, slice based on kept layers, and reshape back\n",
        "                reshaped_params = tensor.reshape((num_layers, tensor.shape[0] // num_layers, tensor.shape[1]))\n",
        "                tensor = reshaped_params[kept_layers_indices, :, :]\n",
        "                tensor = tensor.reshape(-1, tensor.shape[-1]).contiguous()\n",
        "\n",
        "            elif 'embed_tokens_per_layer' in tensor_name:\n",
        "                # Reshape, slice based on kept layers, and reshape back\n",
        "                reshaped_params = tensor.reshape((tensor.shape[0], num_layers, tensor.shape[1] // num_layers))\n",
        "                tensor = reshaped_params[:, kept_layers_indices, :]\n",
        "                tensor = tensor.reshape(tensor.shape[0], -1).contiguous()\n",
        "\n",
        "            # Add the (potentially modified) tensor to the new shard\n",
        "            new_shard_state_dict[new_tensor_name] = tensor\n",
        "\n",
        "            # Check if the current shard is getting too big\n",
        "            current_shard_size = sum(t.numel() * t.element_size() for t in new_shard_state_dict.values())\n",
        "            if current_shard_size > 4000000000: # Create new shard if current is over 4GB\n",
        "                shard_filename = f\"model-{(shard_counter):05d}-of-XXXXX.safetensors\"\n",
        "                print(f\"Saving shard {shard_filename} (size: {current_shard_size / 1e9:.2f} GB)\")\n",
        "                save_file(new_shard_state_dict, os.path.join(local_output_path, shard_filename), metadata={'format': 'pt'})\n",
        "\n",
        "                # Record which tensors are in this shard\n",
        "                for k in new_shard_state_dict.keys():\n",
        "                    weight_map[k] = os.path.basename(shard_filename)\n",
        "\n",
        "                # Reset for the next shard\n",
        "                shard_counter += 1\n",
        "                new_shard_state_dict = {}\n",
        "                gc.collect() # Free up memory\n",
        "    pbar.update(1)\n",
        "\n",
        "pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgN1yAQ-CFkE"
      },
      "outputs": [],
      "source": [
        "# Save any remaining tensors in the last shard\n",
        "if new_shard_state_dict:\n",
        "    shard_filename = f\"model-{(shard_counter):05d}-of-XXXXX.safetensors\"\n",
        "    print(f\"Saving final shard {shard_filename}\")\n",
        "    save_file(new_shard_state_dict, os.path.join(local_output_path, shard_filename), metadata={'format': 'pt'})\n",
        "    for k in new_shard_state_dict.keys():\n",
        "        weight_map[k] = os.path.basename(shard_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZuSMTulGUPv"
      },
      "outputs": [],
      "source": [
        "del new_shard_state_dict\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcEWpUKOGO4A"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "print(\"\\n--- 3. Finalizing Model Save ---\")\n",
        "\n",
        "# The total number of shards we created\n",
        "num_shards = shard_counter\n",
        "\n",
        "# Update the \"XXXXX\" in the filenames to the correct total number of shards\n",
        "for i in range(1, num_shards + 1):\n",
        "    old_filename = f\"model-{(i):05d}-of-XXXXX.safetensors\"\n",
        "    new_filename = f\"model-{(i):05d}-of-{(num_shards):05d}.safetensors\"\n",
        "\n",
        "    # Rename the file\n",
        "    os.rename(os.path.join(local_output_path, old_filename), os.path.join(local_output_path, new_filename))\n",
        "\n",
        "    # Update the weight_map to point to the new filename\n",
        "    for k, v in weight_map.items():\n",
        "        if v == old_filename:\n",
        "            weight_map[k] = new_filename\n",
        "\n",
        "# Create and save the index.json file\n",
        "index_json = {\n",
        "    \"metadata\": {\n",
        "        \"total_size\": sum(os.path.getsize(os.path.join(local_output_path, f)) for f in os.listdir(local_output_path) if f.endswith('.safetensors'))\n",
        "    },\n",
        "    \"weight_map\": weight_map\n",
        "}\n",
        "\n",
        "with open(os.path.join(local_output_path, \"model.safetensors.index.json\"), \"w\") as f:\n",
        "    json.dump(index_json, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ… Model slicing complete. New model saved in: {local_output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1khD_77N8YX"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import  ModelCard, ModelCardData\n",
        "\n",
        "card = ModelCard.load(original_model_id)\n",
        "card.data.base_model = original_model_id\n",
        "del card.data.extra_gated_heading\n",
        "del card.data.extra_gated_prompt\n",
        "card.data.tags.append(\"matformer\")\n",
        "\n",
        "new_description = f\"\"\"\n",
        "> [!Note]\n",
        "> This is a submodel derived from `{original_model_id}`. It has been modified by slicing specific layers and resizing FFN dimensions. It is not the original model.\n",
        "> To learn more about MatFormers, please review the [launch blog](https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide) and generate your own submodels\n",
        "with the [MatFormer Lab](https://goo.gle/gemma3n-matformer-lab).\n",
        ">\n",
        "\n",
        "Skipped layers: {layers_to_skip}\n",
        "\n",
        "FFN hidden dimensions: {ffn_hidden_dims_str}\n",
        "\"\"\"\n",
        "\n",
        "card.text = new_description + \"\\n\" + card.text\n",
        "print(\"Prepended custom description to the model card content.\")\n",
        "\n",
        "new_readme_path = os.path.join(local_output_path, \"README.md\")\n",
        "card.save(new_readme_path)\n",
        "print(f\"New README.md saved to '{new_readme_path}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuF3U6nhOFwu"
      },
      "source": [
        "## Push the model to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hamNb9aGkjP"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "print(f\"Creating private repository: {push_hf_repo_id}\")\n",
        "\n",
        "# Instantiate the HfApi client\n",
        "api = HfApi()\n",
        "\n",
        "# Create a new private repository on the Hub.\n",
        "repo_url = api.create_repo(\n",
        "    repo_id=push_hf_repo_id,\n",
        "    private=True,\n",
        "    exist_ok=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OEIMgDQHCUM"
      },
      "outputs": [],
      "source": [
        "print(f\"Uploading files from '{local_output_path}' to '{push_hf_repo_id}'...\")\n",
        "api.upload_folder(\n",
        "    folder_path=local_output_path,\n",
        "    repo_id=push_hf_repo_id,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=\"Upload sliced model checkpoint\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4Gf-3Tg2TaJs"
      },
      "outputs": [],
      "source": [
        "#@title Verify new model can be loaded\n",
        "\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(push_hf_repo_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "print(f\"Total Parameters: {model.num_parameters():,}\") # 5,976,833,408\n",
        "print(f\"Total Text Parameters: {model.language_model.num_parameters():,}\") # 4,456,156,768\n",
        "print(f\"Effective Parameters (excluding vision, audio, and Per-Layer-Embeddings): {model.language_model.num_parameters(exclude_embeddings=True):,}\") # 1,905,495,648"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO76mJDtOAr7"
      },
      "source": [
        "## Citation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGDUTwIkKrnp"
      },
      "source": [
        "```\n",
        "@article{gemma_3n_2025,\n",
        "    title={Gemma 3n MatFormer Lab},\n",
        "    url={https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/%5BGemma_3n%5DMatFormer_Lab.ipynb},\n",
        "    publisher={Google DeepMind},\n",
        "    author={Puranjay Datta, Aditya Kusupati, Ryan Mullins, Omar Sanseviero, Rakesh Shivanna, Gemma Team},\n",
        "    year={2025}\n",
        "}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "B7r0gnOaIWO3",
        "pytVtFyqJPfp",
        "pjRMQ97hJFtX",
        "1JlVceVlJVG9",
        "3RHu0up8Jp6n"
      ],
      "name": "[Gemma_3n]MatFormer_Lab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a5c51782a30b4e92919d447e98bff14b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5f7f5c50ad74b2eaff0466862d27162",
              "IPY_MODEL_7aa63c3408004f3e80b970afd57978b0",
              "IPY_MODEL_b1f2419836ab47c6bea9292dc48dde2c"
            ],
            "layout": "IPY_MODEL_7a0559c8842d4a3fbea4161adf86c918"
          }
        },
        "a5f7f5c50ad74b2eaff0466862d27162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9d0555e29df4926859fef8ac70e2e3a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e333c06f3a2246859e0fd4ce66926201",
            "value": "Processingâ€‡shards:â€‡â€‡50%"
          }
        },
        "7aa63c3408004f3e80b970afd57978b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_556d541d2cdf4b6a8a40f1268310cd97",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d60e37b41cd43828c762d8746195c54",
            "value": 2
          }
        },
        "b1f2419836ab47c6bea9292dc48dde2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0100bdec1fed489697e9f386c04600fc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a837639fa7b944e992b9fee1033e0385",
            "value": "â€‡2/4â€‡[03:45&lt;04:25,â€‡132.70s/it]"
          }
        },
        "7a0559c8842d4a3fbea4161adf86c918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9d0555e29df4926859fef8ac70e2e3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e333c06f3a2246859e0fd4ce66926201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "556d541d2cdf4b6a8a40f1268310cd97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d60e37b41cd43828c762d8746195c54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0100bdec1fed489697e9f386c04600fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a837639fa7b944e992b9fee1033e0385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}